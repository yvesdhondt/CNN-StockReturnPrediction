{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Intro\n",
    "\n",
    "Before training a model, the raw data for this project needs to be processed. This step assumes that the `/Data` folder is already populated with the data. The data can be found here and is released under CC0: https://www.kaggle.com/datasets/borismarjanovic/price-volume-data-for-all-us-stocks-etfs\n",
    "\n",
    "After downloading the data, you should make sure that the /Data folder contains the following two subfolders: `Data/ETFs` and `Data/Stocks`\n",
    "\n",
    "Remember that the goal of this project is to classify future stock returns based on their past return charts. Therefore the data needs to be transformed so that it can fulfill this goal."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Transform the data\n",
    "\n",
    "The raw data is not suitable for the purposes of this project. Instead the data needs to be transformed into summary statistics (historical returns, standard deviations ...) and stock chart images. The data will be transformed to the following format:\n",
    "\n",
    "- A dataframe where each row corresponds to a single observation. This dataframe contains the following columns:\n",
    "\n",
    "    - `date`: the start date of the observation\n",
    "    - `asset_file`: the path of the file from which this observation was created\n",
    "    - `stock`: a boolean indicator that is 1 if this observation comes from a stock or 0 if it comes from an ETF\n",
    "    - `1_month_return`: the return of the asset in the previous calendar month (lagged by one day to omit look-ahead bias)\n",
    "    - `6_month_return`: the return of the asset in the previous 6 calendar months (lagged by one day to omit look-ahead bias)\n",
    "    - `12_month_return`: the return of the asset in the previous 12 calendar months (lagged by one day to omit look-ahead bias)\n",
    "    - `1_month_volatility`: the (daily) volatility of the asset in the previous calendar month (lagged by one day to omit look-ahead bias)\n",
    "    - `6_month_volatility`: the (daily) volatility of the asset in the previous 6 calendar months (lagged by one day to omit look-ahead bias)\n",
    "    - `12_month_volatility`: the (daily) volatility of the asset in the previous 12 calendar months (lagged by one day to omit look-ahead bias)\n",
    "    - `1_month_img`: the name of the .png file containing a chart of the asset price in the previous calendar month (lagged by one day to omit look-ahead bias)\n",
    "    - `6_month_img`: the name of the .png file containing a chart of the asset price in the previous 6 calendar months (lagged by one day to omit look-ahead bias)\n",
    "    - `12_month_img`: the name of the .png file containing a chart of the asset price in the previous 12 calendar months (lagged by one day to omit look-ahead bias)\n",
    "    - `label`: the classification target. This can be 0 (next-month returns < 1%), 1 (next-month returns >= -1% and <= 1%), or 2 (next-month returns > 1%)\n",
    "\n",
    "- Next to this dataframe, a folder `/Charts` needs to be created that contains all the images referenced in this dataframe."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Supporting functions\n",
    "\n",
    "A number of functions will be set up that will calculate all the features for a single stock at a single point in time. This function can then be applied many times to construct the test, train, & validation data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import os.path\n",
    "\n",
    "import pandas as pd\n",
    "from pandas.errors import EmptyDataError\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import random as rand\n",
    "from datetime import timedelta\n",
    "\n",
    "import pprint as pp # used during code writing, not necessary\n",
    "\n",
    "import matplotlib.pyplot as plt # used to create the images\n",
    "\n",
    "import gc # garbage collector to save on memory\n",
    "\n",
    "from pyts.image import GramianAngularField\n",
    "\n",
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def read_asset_prices(file_path):\n",
    "    \"\"\"\n",
    "    Read the asset prices from the given path\n",
    "    :param file_path: a string containing the path to a csv file with the asset prices\n",
    "    :return: a pandas dataframe with the asset prices for the given asset\n",
    "    \"\"\"\n",
    "    return pd.read_csv(file_path, delimiter=\",\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def get_all_assets():\n",
    "    \"\"\"\n",
    "    Get a list of all the asset price file paths\n",
    "    :return: a list of all the asset price file paths\n",
    "    \"\"\"\n",
    "    stocks = [join(\"Data/Stocks\", f) for f in listdir(\"Data/Stocks\") if isfile(join(\"Data/Stocks\", f))]\n",
    "    etfs = [join(\"Data/ETFs\", f) for f in listdir(\"Data/ETFs\") if isfile(join(\"Data/ETFs\", f))]\n",
    "\n",
    "    return stocks + etfs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def pick_random_asset(asset_list):\n",
    "    \"\"\"\n",
    "    Pick a random asset from the given list\n",
    "    :param asset_list: a list of asset price file paths\n",
    "    :return: a random entry from the given list\n",
    "    \"\"\"\n",
    "\n",
    "    return rand.choice(asset_list)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def calculate_return(asset_prices, start_date, ndays):\n",
    "    \"\"\"\n",
    "    Calculate the 1-day lagged return from ndays ago up until the start date for\n",
    "    the given asset prices. Returns are calculated on closing prices.\n",
    "    :param asset_prices: the asset prices to calculate returns from\n",
    "    :param start_date: the date at which the return is calculated\n",
    "    :param ndays: the number of days to look back\n",
    "    :return: the return over the given period for the given asset\n",
    "    \"\"\"\n",
    "    ndays_price = asset_prices.Close[\n",
    "        asset_prices.Date == asset_prices.Date[asset_prices.Date <= start_date - timedelta(days=ndays)].max()\n",
    "    ].iloc[0]\n",
    "\n",
    "    lagged_price = asset_prices.Close[\n",
    "        asset_prices.Date == asset_prices.Date[asset_prices.Date <= start_date - timedelta(days=1)].max()\n",
    "    ].iloc[0]\n",
    "\n",
    "    return lagged_price / ndays_price - 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def calculate_std(asset_prices, start_date, ndays):\n",
    "    \"\"\"\n",
    "    Calculate the 1-day lagged volatility from ndays ago up until the start date for\n",
    "    the given asset prices. Volatility is calculated on daily returns.\n",
    "    :param asset_prices: the asset prices to calculate returns from\n",
    "    :param start_date: the date at which the return is calculated\n",
    "    :param ndays: the number of days to look back\n",
    "    :return: the volatility over the given period for the given asset\n",
    "    \"\"\"\n",
    "    temp = asset_prices[\n",
    "        (asset_prices.Date <= start_date - timedelta(days=1)) &\n",
    "        (asset_prices.Date >= start_date - timedelta(days=ndays))\n",
    "    ].copy()\n",
    "\n",
    "    temp = temp.sort_values(by=\"Date\", ascending=True)\n",
    "    temp[\"daily_ret\"] = temp.Close / temp.Close.shift(1)\n",
    "    temp.dropna(inplace=True)\n",
    "\n",
    "    return temp.daily_ret.std()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def calculate_next_month_return(asset_prices, start_date):\n",
    "    \"\"\"\n",
    "    Calculate the return for the next month.\n",
    "\n",
    "    :param asset_prices: the asset prices to calculate returns from\n",
    "    :param start_date: the date at which the return is calculated\n",
    "\n",
    "    :return: the return over the next month for the given asset\n",
    "    \"\"\"\n",
    "    next_month_price = asset_prices.Close[\n",
    "        asset_prices.Date == asset_prices.Date[asset_prices.Date <= start_date + timedelta(days=31)].max()\n",
    "    ].iloc[0]\n",
    "\n",
    "    current_price = asset_prices.Close[\n",
    "        asset_prices.Date == start_date\n",
    "    ].iloc[0]\n",
    "\n",
    "    return next_month_price / current_price - 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def calculate_label(ret):\n",
    "    \"\"\"\n",
    "    Calculate the classification label\n",
    "    :param ret: the return to base the calculation on\n",
    "    :return: 0 if ret < -0.01, 1 if 0.01 >= ret >= -0.01, 2 if ret > 0.01\n",
    "    \"\"\"\n",
    "    if ret < -0.01:\n",
    "        return 0\n",
    "    elif ret <= 0.01:\n",
    "        return 1\n",
    "    else:\n",
    "        return 2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def plot_prices(asset_prices, start_date, ndays, img_path):\n",
    "    \"\"\"\n",
    "    Plot the closing prices from ndays ago up until the start date for\n",
    "    the given asset prices on a Gramian Angular Field (2D representation\n",
    "    of time series).\n",
    "\n",
    "    :param asset_prices: the asset prices to plot\n",
    "    :param start_date: the date at which the plot ends\n",
    "    :param ndays: the number of days to look back\n",
    "    :param img_path: the path to save the image to\n",
    "    :return: the location of the created image\n",
    "    \"\"\"\n",
    "    temp = asset_prices[\n",
    "        (asset_prices.Date <= start_date - timedelta(days=1)) &\n",
    "        (asset_prices.Date >= start_date - timedelta(days=ndays))\n",
    "        ].copy()\n",
    "\n",
    "    temp = temp.sort_values(by=\"Date\", ascending=True)\n",
    "    # Smoothen the prices by taking the average of the past 6 days (including the day itself)\n",
    "    # This way, daily random walks of the market are smoothened out more\n",
    "    temp.Close = (temp.Close + temp.Close.shift(1) + temp.Close.shift(2) + temp.Close.shift(3) + temp.Close.shift(4) + temp.Close.shift(5)) / 6\n",
    "    temp.dropna(inplace=True)\n",
    "    temp.reset_index(drop=True, inplace=True)\n",
    "    # Log transform the data as log of prices represent the scaling of returns more accurately\n",
    "    temp.Close = np.log10(temp.Close)\n",
    "    temp.Close = temp.Close - temp.Close.iloc[0] + 1\n",
    "\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=[8, 4])\n",
    "    plt.plot(temp.Date, temp.Close, \"k\") # plot lines in black\n",
    "    plt.axis(\"off\") # turn the axes off\n",
    "    # plt.savefig(img_path)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    plt.close(fig) # avoid displaying image\n",
    "    plt.close(\"all\")\n",
    "    fig.clf() # remove from memory\n",
    "    plt.clf() # clear memory\n",
    "    \"\"\"\n",
    "\n",
    "    data = temp.Close.array.reshape(1, -1) # Reshape to plot on summation field\n",
    "\n",
    "    # get the Gramian Angular Field (auto-scales input data to -1, 1)\n",
    "    gaf = GramianAngularField(method=\"difference\") # The difference field shows directionality in the time series, while the summation field does not\n",
    "    x_gaf = gaf.fit_transform(data)\n",
    "    fig = plt.figure(figsize=(6, 6))\n",
    "    plt.imshow(x_gaf[0], cmap='brg', origin='lower', vmin=-1., vmax=1.) # Choose BRG colormap so the three main color channels are represented\n",
    "    plt.axis(\"off\") # turn the axes off\n",
    "\n",
    "    plt.savefig(img_path)\n",
    "\n",
    "    plt.close(fig) # avoid displaying image\n",
    "    plt.close(\"all\")\n",
    "    fig.clf() # remove from memory\n",
    "    plt.clf() # clear memory\n",
    "\n",
    "    return img_path"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def construct_rand_observation(asset_list, img_prefix=\"\"):\n",
    "    \"\"\"\n",
    "    Create a single random observation\n",
    "    :param asset_list: list of potential asset files to choose from\n",
    "    :param img_prefix: a prefix for the name of the created images\n",
    "    :return: a random observation built from the asset list\n",
    "    \"\"\"\n",
    "\n",
    "    # Pick a random asset\n",
    "    asset_file = pick_random_asset(asset_list)\n",
    "\n",
    "    try:\n",
    "        asset_prices = read_asset_prices(asset_file)\n",
    "    except EmptyDataError:\n",
    "        return {} # Return an empty observation if the given file was empty\n",
    "\n",
    "    try:\n",
    "        # Choose an appropriate time for the observation\n",
    "        asset_prices.Date = pd.to_datetime(asset_prices.Date)\n",
    "\n",
    "        # We need data up to 1 year ago so the chosen data has to be at least\n",
    "        # 1 year from the minimum date for this asset (+ 1 day for the time lag).\n",
    "        # We also need at leas one month of data for the future returns for the\n",
    "        # classification\n",
    "        min_date = asset_prices.Date.min() + timedelta(days=365 + 1)\n",
    "        max_date = asset_prices.Date.max() - timedelta(days=31)\n",
    "\n",
    "        # Choose a start date for the observation\n",
    "        start_date = rand.choice(\n",
    "            asset_prices.Date[\n",
    "                (asset_prices.Date >= min_date) &\n",
    "                (asset_prices.Date < max_date)\n",
    "            ]\n",
    "        )\n",
    "    except IndexError:\n",
    "        # If the date selection failed because of an index error, the asset cannot\n",
    "        # be used so we just return an empty dictionary\n",
    "        return {}\n",
    "    except KeyError:\n",
    "        # If the date selection failed because of an index error, the asset cannot\n",
    "        # be used so we just return an empty dictionary\n",
    "        return {}\n",
    "\n",
    "    try:\n",
    "        observation = {\n",
    "            \"asset_file\": asset_file,\n",
    "            \"date\": start_date,\n",
    "            \"stock\": 1 if \"Stocks\" in asset_file else 0,\n",
    "            \"1_month_return\": calculate_return(asset_prices, start_date, ndays=30),\n",
    "            \"6_month_return\": calculate_return(asset_prices, start_date, ndays=180),\n",
    "            \"12_month_return\": calculate_return(asset_prices, start_date, ndays=365),\n",
    "            \"1_month_volatility\": calculate_std(asset_prices, start_date, ndays=30),\n",
    "            \"6_month_volatility\": calculate_std(asset_prices, start_date, ndays=180),\n",
    "            \"12_month_volatility\": calculate_std(asset_prices, start_date, ndays=365),\n",
    "            \"1_month_img\": plot_prices(asset_prices, start_date, ndays=30, img_path=img_prefix + \"_\" + \"1_month.PNG\"),\n",
    "            \"6_month_img\": plot_prices(asset_prices, start_date, ndays=180, img_path=img_prefix + \"_\" + \"6_month.PNG\"),\n",
    "            \"12_month_img\": plot_prices(asset_prices, start_date, ndays=365, img_path=img_prefix + \"_\" + \"12_month.PNG\"),\n",
    "            \"label\": calculate_label(calculate_next_month_return(asset_prices, start_date))\n",
    "        }\n",
    "    except IndexError:\n",
    "        # If one of the time series did not have enough data to calculate the 1 month image this can occur\n",
    "        return {}\n",
    "\n",
    "    return observation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def read_batches(path):\n",
    "    \"\"\"\n",
    "    Read the batches stored in the path and group them\n",
    "    :param path: the path with batches\n",
    "    :return: a pandas dataframe with the grouped and de-duplicated batches\n",
    "    \"\"\"\n",
    "    # Regroup batches\n",
    "    batches = [pd.read_csv(join(path, f)) for f in listdir(path)]\n",
    "\n",
    "    # After creating the random observations, potential duplicates (although unlikely) need to be dropped\n",
    "    observations = pd.concat(batches)\n",
    "    observations.drop_duplicates(subset=[\"date\", \"asset_file\"])\n",
    "\n",
    "    return observations"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "def construct_many_rand_observations(n=10):\n",
    "    \"\"\"\n",
    "    Create many different observations\n",
    "\n",
    "    :param n: the number of observations to create\n",
    "    :return: a dataframe with the newly created observations (and accompanying image files in a /Charts folder)\n",
    "    \"\"\"\n",
    "    asset_list = get_all_assets()\n",
    "\n",
    "    # Create necessary directories\n",
    "    if not os.path.isdir(\"Charts\"):\n",
    "        os.makedirs(\"Charts\")\n",
    "    if not os.path.isdir(\"DataBatches\"):\n",
    "        os.makedirs(\"DataBatches\")\n",
    "\n",
    "    record_collector = [] # collect observations\n",
    "\n",
    "    # Generate random observations\n",
    "    for i in range(n):\n",
    "\n",
    "        # For memory reasons the pre-processing happens in batches of 1000\n",
    "        if i % 500 == 0 and i != 0:\n",
    "            # Store the observations created so far\n",
    "            observations = pd.DataFrame.from_records(record_collector)\n",
    "            batch_identifier = \"DataBatches/\" + \"rand_observations_\" + str(i) + \".csv\"\n",
    "            observations.to_csv(batch_identifier, index=False)\n",
    "\n",
    "            record_collector = [] # clear the record collector\n",
    "\n",
    "            gc.collect() # run the garbage collector every 1000 observations\n",
    "\n",
    "        # Construct a random observation & save it\n",
    "        obs = {}\n",
    "        while len(obs) == 0: # create random observations until one of them is not empty\n",
    "            obs = construct_rand_observation(asset_list, \"Charts/img_\" + str(i))\n",
    "\n",
    "        record_collector.append(obs)\n",
    "\n",
    "    # Store the final observations if they weren't stored before\n",
    "    if (n-1) % 500 != 0:\n",
    "        observations = pd.DataFrame.from_records(record_collector)\n",
    "        batch_identifier = \"DataBatches/\" + \"rand_observations_\" + str(n-1) + \".csv\"\n",
    "        observations.to_csv(batch_identifier, index=False)\n",
    "\n",
    "        record_collector = None # clear the record collector\n",
    "\n",
    "        gc.collect() # run the garbage collector\n",
    "\n",
    "    # Regroup batches\n",
    "    return read_batches(\"DataBatches\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: out of memory",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mParserError\u001B[0m                               Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-13-3be81b4f63df>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0mrand_observations\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mconstruct_many_rand_observations\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m30000\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32m<ipython-input-12-3316c9eb1e8b>\u001B[0m in \u001B[0;36mconstruct_many_rand_observations\u001B[1;34m(n)\u001B[0m\n\u001B[0;32m     33\u001B[0m         \u001B[0mobs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m{\u001B[0m\u001B[1;33m}\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     34\u001B[0m         \u001B[1;32mwhile\u001B[0m \u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mobs\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m==\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m:\u001B[0m \u001B[1;31m# create random observations until one of them is not empty\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 35\u001B[1;33m             \u001B[0mobs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mconstruct_rand_observation\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0masset_list\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m\"Charts/img_\"\u001B[0m \u001B[1;33m+\u001B[0m \u001B[0mstr\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mi\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     36\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     37\u001B[0m         \u001B[0mrecord_collector\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mobs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<ipython-input-10-08428520498f>\u001B[0m in \u001B[0;36mconstruct_rand_observation\u001B[1;34m(asset_list, img_prefix)\u001B[0m\n\u001B[0;32m     11\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     12\u001B[0m     \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 13\u001B[1;33m         \u001B[0masset_prices\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mread_asset_prices\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0masset_file\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     14\u001B[0m     \u001B[1;32mexcept\u001B[0m \u001B[0mEmptyDataError\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     15\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[1;33m{\u001B[0m\u001B[1;33m}\u001B[0m \u001B[1;31m# Return an empty observation if the given file was empty\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<ipython-input-2-d347e1705c7b>\u001B[0m in \u001B[0;36mread_asset_prices\u001B[1;34m(file_path)\u001B[0m\n\u001B[0;32m      5\u001B[0m     \u001B[1;33m:\u001B[0m\u001B[1;32mreturn\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0ma\u001B[0m \u001B[0mpandas\u001B[0m \u001B[0mdataframe\u001B[0m \u001B[1;32mwith\u001B[0m \u001B[0mthe\u001B[0m \u001B[0masset\u001B[0m \u001B[0mprices\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mthe\u001B[0m \u001B[0mgiven\u001B[0m \u001B[0masset\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      6\u001B[0m     \"\"\"\n\u001B[1;32m----> 7\u001B[1;33m     \u001B[1;32mreturn\u001B[0m \u001B[0mpd\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mread_csv\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfile_path\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdelimiter\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m\",\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\pandas\\util\\_decorators.py\u001B[0m in \u001B[0;36mwrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    309\u001B[0m                     \u001B[0mstacklevel\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mstacklevel\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    310\u001B[0m                 )\n\u001B[1;32m--> 311\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mfunc\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    312\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    313\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0mwrapper\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\pandas\\io\\parsers\\readers.py\u001B[0m in \u001B[0;36mread_csv\u001B[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001B[0m\n\u001B[0;32m    584\u001B[0m     \u001B[0mkwds\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mupdate\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mkwds_defaults\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    585\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 586\u001B[1;33m     \u001B[1;32mreturn\u001B[0m \u001B[0m_read\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfilepath_or_buffer\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mkwds\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    587\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    588\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\pandas\\io\\parsers\\readers.py\u001B[0m in \u001B[0;36m_read\u001B[1;34m(filepath_or_buffer, kwds)\u001B[0m\n\u001B[0;32m    480\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    481\u001B[0m     \u001B[1;31m# Create the parser.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 482\u001B[1;33m     \u001B[0mparser\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mTextFileReader\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfilepath_or_buffer\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwds\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    483\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    484\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0mchunksize\u001B[0m \u001B[1;32mor\u001B[0m \u001B[0miterator\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\pandas\\io\\parsers\\readers.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, f, engine, **kwds)\u001B[0m\n\u001B[0;32m    809\u001B[0m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0moptions\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m\"has_index_names\"\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mkwds\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m\"has_index_names\"\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    810\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 811\u001B[1;33m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_engine\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_make_engine\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mengine\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    812\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    813\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mclose\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\pandas\\io\\parsers\\readers.py\u001B[0m in \u001B[0;36m_make_engine\u001B[1;34m(self, engine)\u001B[0m\n\u001B[0;32m   1038\u001B[0m             )\n\u001B[0;32m   1039\u001B[0m         \u001B[1;31m# error: Too many arguments for \"ParserBase\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1040\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mmapping\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mengine\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mf\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0moptions\u001B[0m\u001B[1;33m)\u001B[0m  \u001B[1;31m# type: ignore[call-arg]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1041\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1042\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m_failover_to_python\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, src, **kwds)\u001B[0m\n\u001B[0;32m     67\u001B[0m         \u001B[0mkwds\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m\"dtype\"\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mensure_dtype_objs\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mkwds\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mget\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"dtype\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     68\u001B[0m         \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 69\u001B[1;33m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_reader\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mparsers\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mTextReader\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mhandles\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mhandle\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwds\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     70\u001B[0m         \u001B[1;32mexcept\u001B[0m \u001B[0mException\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     71\u001B[0m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mhandles\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mclose\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\pandas\\_libs\\parsers.pyx\u001B[0m in \u001B[0;36mpandas._libs.parsers.TextReader.__cinit__\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\pandas\\_libs\\parsers.pyx\u001B[0m in \u001B[0;36mpandas._libs.parsers.TextReader._get_header\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\pandas\\_libs\\parsers.pyx\u001B[0m in \u001B[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\pandas\\_libs\\parsers.pyx\u001B[0m in \u001B[0;36mpandas._libs.parsers.raise_parser_error\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;31mParserError\u001B[0m: Error tokenizing data. C error: out of memory"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rand_observations = construct_many_rand_observations(30000)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "rand_observations"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}